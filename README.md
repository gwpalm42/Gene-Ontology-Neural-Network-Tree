# GONNT - Predict Unknown Gene Function

This project recursively learns paths down the Gene Ontology (http://geneontology.org/) using kmer counting on each gene's corresponding amino acid sequence. This is a more robust method for learning and prediction. Instead of immediatly attempting to predict a leaf, the lowest accuracy term, predicting from the root downwards allows for more accurate predictions on less specific terms. This gives more information about a gene, especially those that are harder to outright predict or have specific leaf functions that are yet to be labelled by the gene ontology. 

This also allows for genes that do not share leaves to constructively contribute to the model's learning. Gene X: (A -> B -> C) and Gene Y: (A -> B -> D) will contribute to the model's understanding of what an A gene at tier one and a B gene at tier two look like, whereas a leaf prediction model would simply learn that X is C and a Y is D and the two genes are not similar. In this model, a gene that is similar to X and Y will receieve the label "A -> B -> ?" where the leaf model would predict "?".

To see how the data was prepared, see runner.ipynb, tree_runner.ipynb, and data_utils.py. To see how the kmers were counted and how the GO paths were generated, see kmer_counter.py and hierarchy_dictionary.py. To see how the model was trained, see network_tree.py for the tree structure and neural_network.py for the individual network training. The threshold_network.py file is not currently implemented in the runner files. This file is an algorithm that allows the neural network to predict multiple labels, rather than just one. This is useful because oftentimes genes will have multiple GO terms associated with them. The next step of this project is to combine the tree path prediction with the multi-output threshold prediction. 

The data for this project (see the raw data folder) was taken from a research project called DeepSeq (see https://github.com/recluze/deepseq). The current predictions.txt file was generated by training the model on all of the data and then predicting the paths for every data point. Because it is predicting on data it has seen, it is likely overperforming.
